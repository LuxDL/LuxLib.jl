{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LuxLib.jl: Backend of Lux.jl","text":""},{"location":"#luxlib","title":"LuxLib","text":"<p>Backend for Lux.jl.</p> <p></p> <p></p>"},{"location":"#api-reference","title":"API Reference","text":""},{"location":"#index","title":"Index","text":"<ul> <li><code>LuxLib.alpha_dropout</code></li> <li><code>LuxLib.batchnorm</code></li> <li><code>LuxLib.dropout</code></li> <li><code>LuxLib.groupnorm</code></li> <li><code>LuxLib.instancenorm</code></li> <li><code>LuxLib.layernorm</code></li> </ul>"},{"location":"#dropout","title":"Dropout","text":"<p># <code>LuxLib.alpha_dropout</code> \u2014 Function.</p> <pre><code>alpha_dropout(rng::AbstractRNG, x, p, ::Val{training})\nalpha_dropout(rng::AbstractRNG, x, p, ::Val{training}, \u03b1, A, B)\n</code></pre> <p>Alpha Dropout: Dropout ensuring that the mean and variance of the output remains same as the input. For details see [1]. Use the second call signature to avoid recomputing the constants for a fixed dropout probability.</p> <p>Arguments</p> <ul> <li><code>rng</code>: Random number generator</li> <li><code>x</code>: Input Array</li> <li><code>p</code>: Probability of an element to be dropped out</li> <li><code>Val(training)</code>: If <code>true</code> then dropout is applied on <code>x</code> with probability <code>p</code>. Else, <code>x</code> is returned</li> <li><code>\u03b1</code>: -1.7580993408473766. Computed at limit x tends to infinity, <code>selu(x) = -\u03bb\u03b2 = \u03b1</code></li> <li><code>A</code>: Scaling factor for the mean</li> <li><code>B</code>: Scaling factor for the variance</li> </ul> <p>Returns</p> <ul> <li>Output Array after applying alpha dropout</li> <li>Updated state for the random number generator</li> </ul> <p>References</p> <p>[1] Klambauer, G\u00fcnter, et al. \"Self-normalizing neural networks.\" Advances in neural     information processing systems 30 (2017).</p> <p>source</p> <p># <code>LuxLib.dropout</code> \u2014 Function.</p> <pre><code>dropout(rng::AbstractRNG, x, p, ::Val{training}; dims, invp=inv(p))\ndropout(rng::AbstractRNG, x, mask, p, ::Val{training}, ::Val{update_mask}; dims,\ninvp=inv(p))\n</code></pre> <p>Dropout: Simple Way to prevent Neural Networks for Overfitting. For details see [1].</p> <p>Arguments</p> <ul> <li><code>rng</code>: Random number generator</li> <li><code>x</code>: Input Array</li> <li><code>mask</code>: Dropout Mask. If not used then it is constructed automatically</li> <li><code>p</code>: Probability of an element to be dropped out</li> <li><code>Val(training)</code>: If <code>true</code> then dropout is applied on <code>x</code> with probability <code>p</code> along <code>dims</code>. Else, <code>x</code> is returned</li> <li><code>Val(update_mask)</code>: If <code>true</code> then the mask is generated and used. Else, the <code>mask</code> provided is directly used</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>dims</code>: Dimensions along which dropout is applied</li> <li><code>invp</code>: Inverse of the probability (\\(\\frac{1}{p}\\))</li> </ul> <p>Returns</p> <ul> <li>Output Array after applying dropout</li> <li>Dropout Mask (if <code>training == false</code>, the returned value is meaningless)</li> <li>Updated state for the random number generator</li> </ul> <p>References</p> <p>[1] Srivastava, Nitish, et al. \"Dropout: a simple way to prevent neural networks from     overfitting.\" The journal of machine learning research 15.1 (2014): 1929-1958.</p> <p>source</p> <p></p> <p></p>"},{"location":"#normalization","title":"Normalization","text":"<p># <code>LuxLib.batchnorm</code> \u2014 Function.</p> <pre><code>batchnorm(x, scale, bias, running_mean, running_var; momentum, epsilon, training)\n</code></pre> <p>Batch Normalization. For details see [1].</p> <p>Batch Normalization computes the mean and variance for each \\(D_1 \\times ... \\times D_{N - 2} \\times 1 \\times D_N\\) input slice and normalises the input accordingly.</p> <p>Arguments</p> <ul> <li><code>x</code>: Input to be Normalized</li> <li><code>scale</code>: Scale factor (\\(\\gamma\\)) (can be <code>nothing</code>)</li> <li><code>bias</code>: Bias factor (\\(\\beta\\)) (can be <code>nothing</code>)</li> <li><code>running_mean</code>: Running mean (can be <code>nothing</code>)</li> <li><code>running_var</code>: Running variance (can be <code>nothing</code>)</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>momentum</code>: Momentum for updating running mean and variance</li> <li><code>epsilon</code>: Value added to the denominator for numerical stability</li> <li><code>training</code>: Set to <code>Val(true)</code> if running in training mode</li> </ul> <p>Returns</p> <p>Normalized Array of same size as <code>x</code>. And a Named Tuple containing the updated running mean and variance.</p> <p>Performance Considerations</p> <p>If the input array is <code>2D</code>, <code>4D</code>, or <code>5D</code> <code>CuArray</code> with element types <code>Float16</code>, <code>Float32</code> and <code>Float64</code>, then the CUDNN code path will be used. In all other cases, a broadcasting fallback is used which is not highly optimized.</p> <p>References</p> <p>[1] Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network     training by reducing internal covariate shift.\" International conference on machine     learning. PMLR, 2015.</p> <p>source</p> <p># <code>LuxLib.groupnorm</code> \u2014 Function.</p> <pre><code>groupnorm(x, scale, bias; groups, epsilon)\ngroupnorm(x, scale, bias, running_mean, running_var; groups, momentum, training,\nepsilon)\n</code></pre> <p>Group Normalization. For details see [1].</p> <p>This op is similar to batch normalization, but statistics are shared across equally-sized groups of channels and not shared across batch dimension. Thus, group normalization does not depend on the batch composition and does not require maintaining internal state for storing statistics.</p> <p>Arguments</p> <ul> <li><code>x</code>: Input to be Normalized</li> <li><code>scale</code>: Scale factor (\\(\\gamma\\)) (can be <code>nothing</code>)</li> <li><code>bias</code>: Bias factor (\\(\\beta\\)) (can be <code>nothing</code>)</li> <li><code>running_mean</code>: Running mean of the inputs. Must be an <code>AV</code> or <code>nothing</code>.</li> <li><code>running_var</code>: Running variance of the inputs. Must be an <code>AV</code> or <code>nothing</code>.</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>groups</code>: Number of groups</li> <li><code>momentum</code>: Momentum for updating running mean and variance.</li> <li><code>training</code>: Set to <code>Val(true)</code> if running in training mode.</li> <li><code>epsilon</code>: Value added to the denominator for numerical stability</li> </ul> <p>Returns</p> <p>If using the first function signature, then the only the normalized array is returned.</p> <p>Otherwise, the normalized array and a named tuple containing updated running mean and updated running variance are returned.</p> <p>Additional Notes</p> <p><code>running_mean</code>, <code>running_var</code>, <code>momentum</code>, and <code>training</code> exist only for backwards compatibility reasons. There is no well documented evidence in literature that tracking statistics for group normalization actually helps. It is recommended to not use these arguments at all.</p> <p>Performance Considerations</p> <p>The most common case of this Op \u2013 <code>x</code> is a 4D array and there is no statistics tracking \u2013 is optimized using KernelAbstractions and has a fast custom backwards pass implemented. All other cases have a fallback implementation which is not especially optimized.</p> <p>Additionally, if the element types of <code>x</code>, <code>scale</code>, and <code>bias</code> are not same and not one of <code>Float32</code> and <code>Float64</code>, then the Op uses the slower fallback implementation. We have tested the code path for <code>Float16</code> and it works, but gradient accumulation is extremely fragile. Hence, for <code>Float16</code> inputs, it uses the fallback implementation.</p> <p>If the batch size is small (&lt; 16), then the fallback implementation will be faster than the KA version. However, this customization is not possible using the direct <code>groupnorm</code> interface.</p> <p>References</p> <p>[1] Wu, Yuxin, and Kaiming He. \"Group normalization.\" Proceedings of the European conference     on computer vision (ECCV). 2018.</p> <p>source</p> <p># <code>LuxLib.instancenorm</code> \u2014 Function.</p> <pre><code>instancenorm(x, scale, bias; epsilon, training)\n</code></pre> <p>Instance Normalization. For details see [1].</p> <p>Instance Normalization computes the mean and variance for each \\(D_1 \\times ... \\times D_{N - 2} \\times 1 \\times 1\\)` input slice and normalises the input accordingly.</p> <p>Arguments</p> <ul> <li><code>x</code>: Input to be Normalized (must be atleast 3D)</li> <li><code>scale</code>: Scale factor (\\(\\gamma\\)) (can be <code>nothing</code>)</li> <li><code>bias</code>: Bias factor (\\(\\beta\\)) (can be <code>nothing</code>)</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>epsilon</code>: Value added to the denominator for numerical stability</li> <li><code>training</code>: Set to <code>Val(true)</code> if running in training mode</li> </ul> <p>Returns</p> <p>Normalized Array of same size as <code>x</code>. And a Named Tuple containing the updated running mean and variance.</p> <p>References</p> <p>[1] Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. \"Instance normalization: The     missing ingredient for fast stylization.\" arXiv preprint arXiv:1607.08022 (2016).</p> <p>source</p> <p># <code>LuxLib.layernorm</code> \u2014 Function.</p> <pre><code>layernorm(x, scale, bias; dims, epsilon)\n</code></pre> <p>Layer Normalization. For details see [1].</p> <p>Given an input array \\(x\\), this layer computes</p> \\[ y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{Var[x] + \\epsilon}} * \\gamma + \\beta \\] <p>Arguments</p> <ul> <li><code>x</code>: Input to be Normalized</li> <li><code>scale</code>: Scale factor (\\(\\gamma\\)) (can be <code>nothing</code>)</li> <li><code>bias</code>: Bias factor (\\(\\beta\\)) (can be <code>nothing</code>)</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>dims</code>: Dimensions along which the mean and std of <code>x</code> is computed</li> <li><code>epsilon</code>: Value added to the denominator for numerical stability</li> </ul> <p>Returns</p> <p>Normalized Array of same size as <code>x</code>.</p> <p>References</p> <p>[1] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. \"Layer normalization.\" arXiv     preprint arXiv:1607.06450 (2016).</p> <p>source</p>"}]}